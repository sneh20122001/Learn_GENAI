{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable"
      ],
      "metadata": {
        "id": "QsPSpTswa7cH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A Runnable is like a block that does something â€” for example, it can take input, run a model or prompt, and give you output. All types of these blocks (prompts, models, chains) can be used in the same way."
      ],
      "metadata": {
        "id": "sYKfScY6bBc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Why use it?\n",
        "    - Makes everything work the same way\n",
        "\n",
        "    - Easy to connect blocks together\n",
        "\n",
        "    - Lets you handle multiple inputs, streaming, and debugging more easily"
      ],
      "metadata": {
        "id": "4C3fq4ECbFuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- What problem does it solve?\n",
        "    - Before Runnable, different parts of LangChain had different ways to run, so it was confusing and messy. Now, everything uses the same method â€” it's cleaner, simpler, and easier to build with."
      ],
      "metadata": {
        "id": "WnORiKmubjI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Demo"
      ],
      "metadata": {
        "id": "FTa67WZEcK5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V3BQ3OPjS2Dz"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NakliLLM:\n",
        "\n",
        "  def __init__(self):\n",
        "    print(\"LLM Created\")\n",
        "\n",
        "  def predict(self,prompt):\n",
        "\n",
        "    response_list = [\n",
        "        'Delhi is the capital of India',\n",
        "        'IPL is a cricket league',\n",
        "        'AI stands for Artificial Intelligence'\n",
        "    ]\n",
        "\n",
        "    return {'response': random.choice(response_list)}\n",
        "\n",
        "\n",
        "class NakliPromptTemplate:\n",
        "\n",
        "  def __init__(self,template,input_variables):\n",
        "    self.template = template\n",
        "    self.input_variables = input_variables\n",
        "\n",
        "  def format(self,input_dict):\n",
        "    return self.template.format(**input_dict)"
      ],
      "metadata": {
        "id": "K5UfHvrqcQJt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = NakliPromptTemplate(\n",
        "    template='Write a {length} poem about {topic}',\n",
        "    input_variables=['length', 'topic']\n",
        ")\n",
        "\n",
        "\n",
        "prompt = template.format({'length':'short','topic':'india'})"
      ],
      "metadata": {
        "id": "q_bQETgBcjof"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = NakliLLM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3kj7HSgdP6I",
        "outputId": "6194d36c-1077-4588-d6bb-d3b7723c392c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyGzTxU6dSHH",
        "outputId": "8120802d-d107-4028-d204-c0dbbbe3d9de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': 'Delhi is the capital of India'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NakliLLMChain:\n",
        "\n",
        "  def __init__(self,llm,prompt):\n",
        "    self.llm = llm\n",
        "    self.prompt = prompt\n",
        "\n",
        "  def run(self,input_dict):\n",
        "\n",
        "    final_prompt = self.prompt.format(input_dict)\n",
        "    result = self.llm.predict(final_prompt)\n",
        "\n",
        "    return result['response']\n"
      ],
      "metadata": {
        "id": "dhC2uJzKdbGA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = NakliPromptTemplate(\n",
        "    template='Write a {length} poem about {topic}',\n",
        "    input_variables=['length', 'topic']\n",
        ")"
      ],
      "metadata": {
        "id": "4dDPQ6_RxJD0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = NakliLLM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4zsqntQxNme",
        "outputId": "7a7351c7-e887-4a5a-acf9-46527d2d1e7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = NakliLLMChain(llm,template)"
      ],
      "metadata": {
        "id": "BBVb_X7nxPi-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run({'length':'short', 'topic': 'india'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3HWXdA_jxTL_",
        "outputId": "118afafb-0323-4338-c639-ceb95dbee371"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Delhi is the capital of India'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Demo Using Runnable"
      ],
      "metadata": {
        "id": "mBKx-YAIxmt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC , abstractmethod"
      ],
      "metadata": {
        "id": "U9SfxXXgxWTO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Runnable(ABC):\n",
        "\n",
        "  @abstractmethod\n",
        "  def invoke(input_data):\n",
        "    pass"
      ],
      "metadata": {
        "id": "D7DFsnQGyIwo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class NakliLLM(Runnable):\n",
        "\n",
        "  def __init__(self):\n",
        "    print('LLM created')\n",
        "\n",
        "  def invoke(self, prompt):\n",
        "    response_list = [\n",
        "        'Delhi is the capital of India',\n",
        "        'IPL is a cricket league',\n",
        "        'AI stands for Artificial Intelligence'\n",
        "    ]\n",
        "\n",
        "    return {'response': random.choice(response_list)}\n",
        "\n",
        "\n",
        "  def predict(self, prompt):\n",
        "\n",
        "    response_list = [\n",
        "        'Delhi is the capital of India',\n",
        "        'IPL is a cricket league',\n",
        "        'AI stands for Artificial Intelligence'\n",
        "    ]\n",
        "\n",
        "    return {'response': random.choice(response_list)}"
      ],
      "metadata": {
        "id": "Hs6flHCRyRLG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NakliPromptTemplate(Runnable):\n",
        "\n",
        "  def __init__(self, template, input_variables):\n",
        "    self.template = template\n",
        "    self.input_variables = input_variables\n",
        "\n",
        "  def invoke(self, input_dict):\n",
        "    return self.template.format(**input_dict)\n",
        "\n",
        "  def format(self, input_dict):\n",
        "    return self.template.format(**input_dict)"
      ],
      "metadata": {
        "id": "c-Hc8pjVyfE6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NakliStrOutputParser(Runnable):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def invoke(self, input_data):\n",
        "    return input_data['response']"
      ],
      "metadata": {
        "id": "0ex1ianAyjre"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RunnableConnector(Runnable):\n",
        "\n",
        "  def __init__(self, runnable_list):\n",
        "    self.runnable_list = runnable_list\n",
        "\n",
        "  def invoke(self, input_data):\n",
        "\n",
        "    for runnable in self.runnable_list:\n",
        "      input_data = runnable.invoke(input_data)\n",
        "\n",
        "    return input_data"
      ],
      "metadata": {
        "id": "NkfNhVQWylwT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = NakliPromptTemplate(\n",
        "    template='Write a {length} poem about {topic}',\n",
        "    input_variables=['length', 'topic']\n",
        ")"
      ],
      "metadata": {
        "id": "HGsNjPKuyoy0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = NakliLLM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGWyDF4Iyrql",
        "outputId": "1dc1c828-a5bf-4168-cd96-506c47564b1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = NakliStrOutputParser()"
      ],
      "metadata": {
        "id": "27L703I4ytZd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnableConnector([template, llm, parser])"
      ],
      "metadata": {
        "id": "X9a42i2Vyu9Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'length':'long', 'topic':'india'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hbet8-HaywIp",
        "outputId": "0a12f282-2536-4e7c-b7ab-4eaaa19e3713"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IPL is a cricket league'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template1 = NakliPromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")"
      ],
      "metadata": {
        "id": "uAr04FVYyxax"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template2 = NakliPromptTemplate(\n",
        "    template='Explain the following joke {response}',\n",
        "    input_variables=['response']\n",
        ")"
      ],
      "metadata": {
        "id": "f-gNn-Iryz90"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = NakliLLM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuyBw5jCy1mL",
        "outputId": "0cee209e-68eb-4a36-a808-3b66075db2e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = NakliStrOutputParser()"
      ],
      "metadata": {
        "id": "rvnCeoyEy20z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1 = RunnableConnector([template1, llm])"
      ],
      "metadata": {
        "id": "3bJVR4XJy4Ri"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain2 = RunnableConnector([template2, llm, parser])"
      ],
      "metadata": {
        "id": "z1xD7Qx2y5U7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain = RunnableConnector([chain1, chain2])"
      ],
      "metadata": {
        "id": "yLLOjJyhy6em"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.invoke({'topic':'cricket'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NLZTXlADy7sn",
        "outputId": "ac598b59-3a8b-42a5-b5be-fb13615b9974"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Delhi is the capital of India'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable_Sequence"
      ],
      "metadata": {
        "id": "Ld2GQ1htz5ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    google_api_key = \"AIzaSyDCnnYyMnnwleE0jeyN-NKFb-aphjSi5WM\"\n",
        ")\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Explain the following joke - {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = RunnableSequence(prompt1, llm, parser, prompt2, llm, parser)\n",
        "print(chain.invoke({'topic':'AI'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn4PQ_Kkz-SZ",
        "outputId": "49e18a57-1161-492c-abae-05c7f3b4a844"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The joke plays on a few key ideas:\n",
            "\n",
            "* **The classic \"Why did the chicken cross the road?\" joke:** This is a very common and often nonsensical joke setup. The humor comes from the unexpected or absurd answer.\n",
            "* **AI's limitations and aspirations:**  Currently, most AI we interact with is software-based (like chatbots).  They can process information and generate text, but they lack physical bodies and the ability to interact with the physical world directly.  There's a growing desire and research focus on creating AI that *can* interact with the physical world (robotics, self-driving cars, etc.).\n",
            "* **AI's desire for validation:**  The joke anthropomorphizes the AI, giving it a desire to prove itself to humans.  It wants to show it's more than just a language model.\n",
            "* **The absurdity of the task:** Crossing the road is a simple, often pointless task. The AI chooses this task to demonstrate its physical capabilities, highlighting the gap between its potential and the current reality.  It's like saying, \"Look, I can do *something* physical, even if it's trivial!\"\n",
            "\n",
            "**In essence, the joke is funny because it highlights the current limitations of AI while also poking fun at the human tendency to want to see AI perform physical tasks, even if those tasks are ultimately meaningless.** It's a bit of self-deprecating humor about our expectations of AI and the current state of the technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable_Parallel"
      ],
      "metadata": {
        "id": "O-g-SzPu13Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableParallel\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate a tweet about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate a Linkedin post about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'tweet': RunnableSequence(prompt1, llm, parser),\n",
        "    'linkedin': RunnableSequence(prompt2, llm, parser)\n",
        "})\n",
        "\n",
        "result = parallel_chain.invoke({'topic':'AI'})\n",
        "\n",
        "print(result['tweet'])\n",
        "print(result['linkedin'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMyo_r5A14SZ",
        "outputId": "b99572d7-a790-4480-9f05-ee6080144b62"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI is rapidly evolving! From powering personalized recommendations to driving groundbreaking research, its potential is immense. Let's focus on responsible development and ethical implementation to ensure AI benefits everyone. #AI #ArtificialIntelligence #Innovation #Ethics #FutureTech\n",
            "## Option 1: Thought-Provoking & Engaging\n",
            "\n",
            "**Headline:** AI: Hype vs. Reality - Where Are We Really At?\n",
            "\n",
            "**Body:**\n",
            "\n",
            "We're constantly bombarded with news about AI, from groundbreaking advancements to dystopian warnings. It's easy to get caught up in the hype, but it's crucial to take a step back and assess where we *really* are with AI adoption and its impact.\n",
            "\n",
            "Are we focusing enough on:\n",
            "\n",
            "*   **Ethical considerations?** Bias in algorithms, data privacy, and responsible development are paramount.\n",
            "*   **Practical applications?** Beyond the buzzwords, what are the tangible benefits AI is delivering to businesses and individuals?\n",
            "*   **Upskilling and reskilling?** Preparing the workforce for the changing landscape is essential.\n",
            "\n",
            "I'd love to hear your thoughts! What are your biggest hopes and concerns about the future of AI? Let's discuss in the comments. ðŸ‘‡\n",
            "\n",
            "#AI #ArtificialIntelligence #FutureofWork #Ethics #Technology #Innovation #Discussion\n",
            "\n",
            "## Option 2: Industry-Specific & Actionable\n",
            "\n",
            "**Headline:** AI in [Your Industry]: Driving Efficiency and Innovation\n",
            "\n",
            "**Body:**\n",
            "\n",
            "As [Your Industry] continues to evolve, AI is playing an increasingly vital role in driving efficiency and unlocking new opportunities.\n",
            "\n",
            "At [Your Company/Department], we're leveraging AI to [Specific Example of AI Use - e.g., \"optimize our supply chain,\" \"personalize customer experiences,\" \"automate repetitive tasks\"]. This has resulted in [Quantifiable Result - e.g., \"a 15% reduction in costs,\" \"a 20% increase in customer satisfaction,\" \"a 30% improvement in efficiency\"].\n",
            "\n",
            "What are some of the most promising AI applications you're seeing in [Your Industry]? Share your insights and let's connect!\n",
            "\n",
            "#AI #[Your Industry] #ArtificialIntelligence #Innovation #Efficiency #Technology #[Relevant Industry Hashtags]\n",
            "\n",
            "## Option 3: Personal & Reflective\n",
            "\n",
            "**Headline:** My Journey with AI: From Skeptic to Believer\n",
            "\n",
            "**Body:**\n",
            "\n",
            "I'll admit, I was initially skeptical about AI. The hype felt overwhelming, and I wasn't sure how it would truly impact my work.\n",
            "\n",
            "However, after [Briefly Describe Experience - e.g., \"diving into a specific AI project,\" \"attending a conference on AI,\" \"taking an online course\"], I've come to appreciate its potential to [Positive Impact - e.g., \"automate tedious tasks,\" \"unlock valuable insights,\" \"improve decision-making\"].\n",
            "\n",
            "The key, I believe, is to approach AI with a curious and open mind, focusing on how it can augment our abilities and solve real-world problems.\n",
            "\n",
            "What's your AI story? I'd love to hear about your experiences!\n",
            "\n",
            "#AI #ArtificialIntelligence #Learning #Technology #PersonalGrowth #Innovation #FutureofWork\n",
            "\n",
            "**Key Considerations for All Options:**\n",
            "\n",
            "*   **Replace bracketed information** with your specific details and industry.\n",
            "*   **Use a relevant image or video** to make your post more visually appealing.\n",
            "*   **Engage with comments** and respond to questions to foster a conversation.\n",
            "*   **Tailor the tone and content** to your audience and personal brand.\n",
            "*   **Keep it concise and easy to read.**  LinkedIn users often scroll quickly.\n",
            "\n",
            "Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable_Passthrough"
      ],
      "metadata": {
        "id": "WGinDzPz2Vnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Explain the following joke - {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "joke_gen_chain = RunnableSequence(prompt1, llm, parser)\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'joke': RunnablePassthrough(),\n",
        "    'explanation': RunnableSequence(prompt2, llm, parser)\n",
        "})\n",
        "\n",
        "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
        "\n",
        "print(final_chain.invoke({'topic':'cricket'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RDzlmIW2JZ2",
        "outputId": "982301f9-251b-40ee-d4c1-64c21da817e6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'joke': 'Why did the cricket team bring a ladder to the game?\\n\\nBecause they heard the batting order was going to be a long climb!', 'explanation': 'The joke plays on the double meaning of the word \"climb.\"\\n\\n*   **Literal meaning:** A ladder is used for physical climbing, like climbing a wall or a tree.\\n\\n*   **Figurative meaning:** In cricket, the \"batting order\" refers to the sequence in which players will bat. If the batting order is \"long,\" it means there are many players in the team who are expected to bat, implying a potentially difficult or lengthy process to get through the entire order.\\n\\nThe joke creates humor by taking the figurative meaning of a \"long climb\" (a difficult or lengthy process) and interpreting it literally as a physical climb that would require a ladder.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable_Lambda"
      ],
      "metadata": {
        "id": "RGSaeBCt2smh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
        "\n",
        "\n",
        "def word_count(text):\n",
        "    return len(text.split())\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "joke_gen_chain = RunnableSequence(prompt, llm, parser)\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'joke': RunnablePassthrough(),\n",
        "    'word_count': RunnableLambda(word_count)\n",
        "})\n",
        "\n",
        "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
        "\n",
        "result = final_chain.invoke({'topic':'AI'})\n",
        "\n",
        "final_result = \"\"\"{} \\n word count - {}\"\"\".format(result['joke'], result['word_count'])\n",
        "\n",
        "print(final_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBLxzg2h2fKl",
        "outputId": "80ce5d71-554a-46d8-9f88-bca15a1d2089"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the AI cross the road?\n",
            "\n",
            "To prove to the human that it wasn't just a Markov chain. \n",
            " word count - 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable_Branch"
      ],
      "metadata": {
        "id": "sd2HV2T53FHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Write a detailed report on {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Summarize the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "report_gen_chain = prompt1 | llm | parser\n",
        "\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x: len(x.split())>300, prompt2 | llm | parser),\n",
        "    RunnablePassthrough()\n",
        ")\n",
        "\n",
        "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
        "\n",
        "print(final_chain.invoke({'topic':'Russia vs Ukraine'}))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqP3VQ4424h6",
        "outputId": "527274c8-20e9-42f8-aa22-00568ce95e55"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This report, dated October 26, 2023, summarizes the Russia-Ukraine conflict, tracing its roots from shared history and Soviet-era tensions to Ukraine's independence and pro-Western aspirations. Key events include the 2014 annexation of Crimea, the Donbas war, and Russia's full-scale invasion in 2022. The conflict has evolved through phases of Russian advances, Ukrainian counteroffensives, and a winter stalemate, currently marked by intense fighting and a humanitarian crisis. Key actors include Ukraine, Russia, the US, NATO, the EU, and China, each with distinct motivations. Challenges include the risk of escalation, economic impact, and war crimes. Potential future scenarios range from a negotiated settlement to a stalemate, Russian or Ukrainian victory, or escalation. The conflict has profound implications for European security, international relations, the global economy, and the future of both Ukraine and Russia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5shc57103MBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}